<!DOCTYPE html> <html lang="en"> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JEZS4MT5D"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5JEZS4MT5D");</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Dr. Cong Zhang</title> <meta name="author" content="Cong Zhang"/> <meta name="description" content="I will try my best to keep this page updated."/> <meta name="keywords" content="cong, cong zhang, newcaslte, oxford, linguistics, prosody, phonetics, phonology, speech"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icon.jpg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://congzhang-linguist.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Dr. Cong Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">awards</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">I will try my best to keep this page updated.</p> </header> <article> <div class="publications"> <h1>Articles</h1> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">tone</abbr></div> <div id="xu2024across" class="col-sm-8"> <div class="title">A cross-linguistic review of citation tone production studies: Methodology and recommendations</div> <div class="author"> Chenzi Xu, and <em>Cong Zhang</em> </div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1121/10.0032356" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://pubs.aip.org/asa/jasa/article-pdf/156/4/2538/20213761/2538_1_10.0032356.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The study of citation tones, lexical tones produced in isolation, is one of the first steps towards understanding speech prosody in tone languages. However, methodologies for investigating citation tones vary significantly, often leading to limited comparability of tone inventories, both within and across languages. This paper presents a systematic review of research methods and practices in 136 citation tone studies on 129 tonal language varieties in China, including 99 studies published in Chinese, which are therefore not easily available to an international scientific readership. The review provides an overview of possible analytical decisions along the research pipeline, and unveils considerable variation in data collection, analysis, and reporting conventions, particularly in how f0, the primary acoustic correlate for tone, is operationalised and reported across studies. Key methodological issues are identified, including small sample sizes and inadequate transparency in communicating methodological decisions and procedure. This paper offers a clear road map for citation tone production research and proposes a range of recommendations on speaker sampling, experimental design, acoustic processing techniques, f0 analysis, and result reporting, with the goal of facilitating future tonal research and enhancing resources for underrepresented tonal varieties.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">remote</abbr></div> <div id="zhang2024investigating" class="col-sm-8"> <div class="title">Investigating differences in lab-quality and remote recording methods with dynamic acoustic measures</div> <div class="author"> <em>Cong Zhang</em>, Kathleen Jepson, and Yu-Ying Chuang</div> <div class="periodical"> <em>Laboratory Phonology</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.journal-labphon.org/article/10492/galley/33869/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.journal-labphon.org/article/10492/galley/33869/download/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Increasingly, phonetic research uses data collected from participants who record themselves on readily available devices. Though such recordings are convenient, their suitability for acoustic analysis remains an open question, especially regarding how recording methods affect acoustic measures over time. We used Quantile Generalized Additive Mixed Models (QGAMMs) to analyze measures of F0, intensity, and the first and second formants, comparing files recorded using a laboratory-standard recording method (Zoom H6 recorder with an external microphone), to three remote recording methods: (1) the Awesome Voice Recorder application on a smartphone (AVR), (2) the Zoom meeting application with default settings (Zoom-default), and (3) the Zoom meeting application with the “Turn on Original Sound” setting (Zoom-raw). A linear temporal alignment issue was observed for the Zoom methods over the course of the long, recording session files; however, the difference was not significant for utterance-length files. F0 was reliably measured using all methods. Intensity and formants presented non-linear differences across methods that could not be corrected for simply. Overall, the AVR files were most similar to the H6’s, and so AVR is deemed to be a more reliable recording method than either Zoom-default or Zoom-raw.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2024investigating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Investigating differences in lab-quality and remote recording methods with dynamic acoustic measures}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Jepson, Kathleen and Chuang, Yu-Ying}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Laboratory Phonology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.16995/labphon.10492}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">aphasia</abbr></div> <div id="zhang2024aphasia" class="col-sm-8"> <div class="title">Prosody of speech production in latent post-stroke aphasia </div> <div class="author"> <em>Cong Zhang</em>, Tong Li, Gayle DeDe, and Christos Salis</div> <div class="periodical"> <em>In Interspeech 2024</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ZhangEtAl_2024_Interspeech.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This study explores prosodic production in latent aphasia, a mild form of aphasia associated with left-hemisphere brain damage (e.g. stroke). Unlike prior research on moderate to severe aphasia, we investigated latent aphasia, which can seem to have very similar speech production with neurotypical speech. We analysed the f0, intensity and duration of utterance-initial and utterance-final words of ten speakers with latent aphasia and ten matching controls. Regression models were fitted to improve our understanding of this understudied type of very mild aphasia. The results highlighted varying degrees of differences in all three prosodic measures between groups. We also investigated the diagnostic classification of latent aphasia versus neurotypical control using random forest, aiming to build a fast and reliable tool to assist with the identification of latent aphasia. The random forest analysis also reinforced the significance of prosodic features in distinguishing latent aphasia.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024aphasia</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prosody of speech production in latent post-stroke aphasia }</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Li, Tong and DeDe, Gayle and Salis, Christos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">gamification</abbr></div> <div id="kim2024collecting" class="col-sm-8"> <div class="title">Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics</div> <div class="author"> Yoolim Kim, Vita V Kogan, and <em>Cong Zhang</em> </div> <div class="periodical"> <em>Applied Linguistics</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1093/applin/amad039" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://academic.oup.com/applij/advance-article/doi/10.1093/applin/amad039/7223350?utm_source=authortollfreelink&amp;utm_campaign=applij&amp;utm_medium=email&amp;guestAccessKey=6ebc0401-70dd-4539-982e-25e6608e3a34&amp;login=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Gamification of behavioral experiments has been applied successfully to research in a number of disciplines, including linguistics. We believe that these methods have been underutilized in applied linguistics, in particular second-language acquisition research. The incorporation of games and gaming elements (gamification) in behavioral experiments has been shown to mitigate many of the practical constraints characteristic of lab settings, such as limited recruitment or only achieving small-scale data. However, such constraints are no longer an issue with gamified and game-based experiments, and as a result, data collection can occur remotely with greater ease and on a much wider scale, yielding data that are ecologically valid and robust. These methods enable the collection of data that are comparable in quality to the data collected in more traditional settings while engaging far more diverse participants with different language backgrounds that are more representative of the greater population. We highlight three successful applications of using games and gamification with applied linguistic experiments to illustrate the effectiveness of such approaches in a greater effort to invite other applied linguists to do the same.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2024collecting</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Yoolim and Kogan, Vita V and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Linguistics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{198-205}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0142-6001}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/applin/amad039}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">clinical</abbr></div> <div id="zengExploringAcousticProsodic2023" class="col-sm-8"> <div class="title">Exploring the Acoustic and Prosodic Features of a Lung-Function-Sensitive Repeated-Word Speech Articulation Test</div> <div class="author"> Biao Zeng, Edgar Mark Williams, Chelsea Owen, <em>Cong Zhang</em>, Shakiela Khanam Davies, Keira Evans, and Savannah-Rose Preudhomme</div> <div class="periodical"> <em>Frontiers in Psychology</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1167902/full" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1167902/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>IntroductionSpeech breathing is a term usually used to refer to the manner in which expired air and lung mechanics are utilized for the production of the airflow necessary for phonation. Neurologically, speech breathing overrides the normal rhythms of alveolar ventilation. Speech breathing is generated using the diaphragm, glottis, and tongue. The glottis is the opening between the vocal folds in the larynx; it is the primary valve between the lungs and the mouth, and by varying its degree of opening, the sound can be varied. The use of voice as an indicator of health has been widely reported. Chronic obstructive pulmonary disease (COPD) is the most common long-term respiratory disease. The main symptoms of COPD are increasing breathlessness, a persistent chesty cough with phlegm, frequent chest infections, and persistent wheezing. There is no cure for COPD, and it is one of the leading causes of death worldwide. The principal cause of COPD is tobacco smoking, and estimates indicate that COPD will become the third leading cause of death worldwide by 2030. The long-term aim of this research program is to understand how speech generation, breathing, and lung function are linked in people with chronic respiratory diseases such as COPD.MethodsThis pilot study was designed to test an articulatory speech task that uses a single word (“helicopter”), repeated multiple times, to challenge speech-generated breathing and breathlessness. Specifically, a single-word articulation task was used to challenge respiratory system endurance in people with healthy lungs by asking participants to rapidly repeat the word “helicopter” for three 20-s runs interspersed with two 20-s rest periods of silent relaxed breathing. Acoustic and prosodic features were then extracted from the audio recordings of each adult participant.Results and discussionThe pause ratio increased from the first run to the third, representing an increasing demand for breath. These data show that the repeated articulation task challenges speech articulation in a quantifiable manner, which may prove useful in defining respiratory ill-health.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zengExploringAcousticProsodic2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring the Acoustic and Prosodic Features of a Lung-Function-Sensitive Repeated-Word Speech Articulation Test}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zeng, Biao and Williams, Edgar Mark and Owen, Chelsea and Zhang, Cong and Davies, Shakiela Khanam and Evans, Keira and Preudhomme, Savannah-Rose}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Psychology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1664-1078}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">BigTeam</abbr></div> <div id="coretta2023multidimensional" class="col-sm-8"> <div class="title">Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses</div> <div class="author"> Ste Coretta, Joseph V Casillas,  ..., <em>Cong Zhang</em>,  ..., and Timo B Roettger</div> <div class="periodical"> <em>Advances in Methods and Practices in Psychological Sciences</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1177/25152459231162567" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://journals.sagepub.com/doi/pdf/10.1177/25152459231162567?download=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis which can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling, but also from decisions regarding the quantification of the measured behavior. In the present study, we gave the same speech production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further find little to no evidence that the observed variability can be explained by analysts’ prior beliefs, expertise or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system and calibrate their (un)certainty in their conclusions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">coretta2023multidimensional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Coretta, Ste and Casillas, Joseph V and ... and Zhang, Cong and ... and Roettger, Timo B}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Methods and Practices in Psychological Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICPhS</abbr></div> <div id="zhang2023language-redundancy" class="col-sm-8"> <div class="title">Language redundancy effects on F0: A preliminary controlled study</div> <div class="author"> <em>Cong Zhang</em>, Catherine Lai, Ricardo Souza, Alice Turk, and Tina Bögel</div> <div class="periodical"> <em>In Proceeding of 20th International Congress of Phonetic Sciences</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="html" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/ZhangEtAl_2023_ICPhS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/ZhangEtAl_2023_ICPhS_poster.pdf" class="btn btn-sm z-depth-0" role="button">Presentation</a> </div> <div class="abstract hidden"> <p>Previous research suggests that words with a high level of language redundancy (i.e. recognition likelihood from familiarity and predictability based on syntactic, pragmatic, and semantic factors) have reduced acoustic salience, such as shorter duration and reduced vowels. The Smooth Signal Redundancy Hypothesis proposes that acoustic salience is controlled via prosodic structure, and makes the prediction that parameters such as fundamental frequency should also be affected by language redundancy. This study investigates the relationship of F0 with lexical frequency, together with bigram (verb-adjective or adjective-noun) frequency and the ratio between these two bigram frequencies. Results from a carefully controlled experiment with quadruplets of minimal pairs suggests that language redundancy can affect fundamental frequency in English.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023language-redundancy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Language redundancy effects on F0: A preliminary controlled study}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Lai, Catherine and Napoleão de Souza, Ricardo and Turk, Alice and Bögel, Tina}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceeding of 20th International Congress of Phonetic Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP Findings</abbr></div> <div id="zhu2022bootstrapping" class="col-sm-8"> <div class="title">Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings</div> <div class="author"> Jian Zhu, Zuoyu Tian, Yadong Liu, <em>Cong Zhang</em>, and Chia-wen Lo</div> <div class="periodical"> <em>Findings of Empirical Methods in Natural Language Processing</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.findings-emnlp.81/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://aclanthology.org/2022.findings-emnlp.81.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/spoken_sent_embedding" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5 0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2022bootstrapping</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Jian and Tian, Zuoyu and Liu, Yadong and Zhang, Cong and Lo, Chia-wen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Findings of Empirical Methods in Natural Language Processing}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">rhythm</abbr></div> <div id="sun2022task" class="col-sm-8"> <div class="title">Task effect on L2 rhythm production by Cantonese learners of Portuguese</div> <div class="author"> Yuqi Sun, and <em>Cong Zhang</em> </div> <div class="periodical"> <em>DELTA: Documentação de Estudos em Lingüística Teórica e Aplicada</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1590/1678-460X202258943" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.scielo.br/j/delta/a/QwGKgDkWkvJbNsZ9CrgZNvb/?format=pdf&amp;lang=en" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/spoken_sent_embedding" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This study examines L2 Portuguese speech produced by eight native Cantonese speakers from Macao, China. The aims of this study are to investigate (1) whether the speech rhythm in L2 Portuguese is more source-like (more similar to Cantonese) or more target-like (more similar to Portuguese), and (2) whether L2 speech rhythm differs across three different tasks: a reading task, a retelling task, and an interpreting task. Seven rhythm metrics, i.e., %V, ΔC, ΔV, VarcoC, VarcoV, rPVI_C, and nPVI_V, were adopted for comparison and investigation. The results showed that L2 Portuguese rhythm produced by Cantonese speakers differed from L1 Portuguese speakers’ rhythm. R-deletion and vowel epenthesis were the reasons for the variabilities and instabilities of L2 Portuguese production by Cantonese learners, as they affect the duration and the number of vowel intervals and consonantal intervals. Moreover, in Cantonese learners’ L2 Portuguese production, the semi-spontaneous tasks (retelling and interpreting) presented a significant difference from the reading task. The driving force for such a difference was the cognitive load behind the tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sun2022task</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sun, Yuqi and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task effect on L2 rhythm production by Cantonese learners of Portuguese}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0102-4450}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1590/1678-460X202258943}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{DELTA: Documentação de Estudos em Lingüística Teórica e Aplicada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pontifícia Universidade Católica de São Paulo - PUC-SP}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{202258943}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="zhu2022byt5-g2p" class="col-sm-8"> <div class="title">ByT5 model for massively multilingual grapheme-to-phoneme conversion</div> <div class="author"> Jian Zhu*, Cong Zhang*, and David Jurgens [* equal contribution]</div> <div class="periodical"> <em>In Interspeech 2022</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.03067.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/CharsiuG2P" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2022byt5-g2p</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ByT5 model for massively multilingual grapheme-to-phoneme conversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{Jian Zhu*, Cong Zhang*} and {[* equal contribution]}, David Jurgens}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICASSP</abbr></div> <div id="zhu2022phone-charsiu" class="col-sm-8"> <div class="title">Phone-to-audio alignment without text: A Semi-supervised Approach</div> <div class="author"> Jian Zhu, <em>Cong Zhang</em>, and David Jurgens</div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1109/ICASSP43922.2022.9746112" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9746112" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/charsiu" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The task of phone-to-audio alignment has many applications in speech research. Here we introduce two Wav2Vec2-based models for both text-dependent and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a semi-supervised model, directly learns phone-to-audio alignment through contrastive learning and a forward sum loss, and can be coupled with a pretrained phone recognizer to achieve text-independent alignment. The other model, Wav2Vec2-FC, is a frame classification model trained on forced aligned labels that can both perform forced alignment and text-independent segmentation. Evaluation results suggest that both proposed methods, even when transcriptions are not available, generate highly close results to existing forced alignment tools. Our work presents a neural pipeline of fully automated phone-to-audio alignment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2022phone-charsiu</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Phone-to-audio alignment without text: A Semi-supervised Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Jian and Zhang, Cong and Jurgens, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Acoustics, Speech and Signal Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="gryllia2022many-shape" class="col-sm-8"> <div class="title">The many shapes of H*</div> <div class="author"> Stella Gryllia, Amalia Arvaniti, <em>Cong Zhang</em>, and Katherine Marcoux</div> <div class="periodical"> <em>In Speech Prosody 2022</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.21437/SpeechProsody.2022-153" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-speech.org/archive/pdfs/speechprosody_2022/gryllia22_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/emcfg/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We examined individual and task-related variability in the realization of Greek nuclear H* followed by L-L% edge tones. The accents (N = 748) were elicited from native speakers of Greek, producing scripted and unscripted speech, and examined using functional Principal Components Analysis. The accented vowel onset was used for landmark registration to capture accent shape and the alignment of the fall. The resulting PCs were analysed using LMEMs (fixed factors: speaker; task type (scripted, unscripted); accented syllable distance from the analysis window offset, to examine the effects of tonal crowding). Tonal scaling and the steepness of the fall (reflected in PC1 and PC2 respectively) changed by task in ways that differed across speakers. PC3, which captured accent shape, also varied by speaker, reflecting shape differences between a rise-fall and (the expected) plateau-plus-fall realization. Tonal crowding did not have consistent effects. In short, the overall accent shape and the alignment of the accentual fall varied by speaker and task. These results hint at substantial variability in tonal realization. At the same time, they indicate that tonal alignment is not as consistent as is sometimes portrayed and thus it should not be the sole criterion for tone categorization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gryllia2022many-shape</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gryllia, Stella and Arvaniti, Amalia and Zhang, Cong and Marcoux, Katherine}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Speech Prosody 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The many shapes of H*}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="arvaniti2022disentangling" class="col-sm-8"> <div class="title">Disentangling emphasis from pragmatic contrastivity in the English H* ∼L+H* contrast</div> <div class="author"> Amalia Arvaniti, Stella Gryllia, <em>Cong Zhang</em>, and Katherine Marcoux</div> <div class="periodical"> <em>In Speech Prosody 2022</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.21437/SpeechProsody.2022-170" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-speech.org/archive/pdfs/speechprosody_2022/arvaniti22_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/wm7bc/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>English H* and L+H* indicate new and contrastive information respectively, though some argue the difference between them is solely one of phonetic emphasis. We used (modified) Rapid Prosody Transcription to test these views. Forty-seven speakers of Standard Southern British English (SSBE) listened to 86 SSBE utterances and marked the words they considered prominent or emphatic. Accents (N = 281) were independently coded as H* or L+H* using phonetic criteria, and as contrastive or non-contrastive using pragmatic criteria. If L+H* is an emphatic H*, all L+H*s should be more prominent than H*s. If the accents mark pragmatic information, contrastivity should drive responses. Contrastive accents and L+H*s were considered more prominent than non-contrastive accents and H*s respectively. Individual responses showed different strategies: for some participants, all L+H*s were more prominent than H*s, for others, contrastive accents were more prominent than non-contrastive accents, and for still others, there was no difference between categories. These results indicate that a reason for the continuing debate about English H* and L+H* may be that the two accents form a weak contrast which some speakers acquire and attend to while others do not.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">arvaniti2022disentangling</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arvaniti, Amalia and Gryllia, Stella and Zhang, Cong and Marcoux, Katherine}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Speech Prosody 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Disentangling emphasis from pragmatic contrastivity in the English H* $\sim$ L+H* contrast}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="zhang2021synchronising" class="col-sm-8"> <div class="title">Synchronising Speech Segments with Musical Beats in Mandarin and English Singing</div> <div class="author"> <em>Cong Zhang</em>, and Jian Zhu</div> <div class="periodical"> <em>In Interspeech 2021</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21i_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-archive.org/interspeech_2021/zhang21i_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/8m5bj/?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Generating synthesised singing voice with models trained on speech data has many advantages due to the models’ flexibility and controllability. However, since the information about the temporal relationship between segments and beats are lacking in speech training data, the synthesised singing may sound off-beat at times. Therefore, the availability of the information on the temporal relationship between speech segments and music beats is crucial. The current study investigated the segment-beat synchronisation in singing data, with hypotheses formed based on the linguistics theories of P-centre and sonority hierarchy. A Mandarin corpus and an English corpus of professional singing data were manually annotated and analysed. The results showed that the presence of musical beats was more dependent on segment duration than sonority. However, the sonority hierarchy and the P-centre theory were highly related to the location of beats. Mandarin and English demonstrated cross-linguistic variations despite exhibiting common patterns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2021synchronising</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Zhu, Jian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Synchronising Speech Segments with Musical Beats in Mandarin and English Singing}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1199--1203}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2021-1841}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JASA</abbr></div> <div id="zhang2021comparing" class="col-sm-8"> <div class="title">Comparing acoustic analyses of speech data collected remotely</div> <div class="author"> <em>Cong Zhang</em>, Kathleen Jepson, Georg Lohfink, and Amalia Arvaniti</div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pubs.aip.org/asa/jasa/article/149/6/3910/1059288/Comparing-acoustic-analyses-of-speech-data" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8269758/pdf/JASMAN-000149-003910_1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Face-to-face speech data collection has been next to impossible globally due to COVID-19 restrictions. To address this problem, simultaneous recordings of three repetitions of the cardinal vowels were made using a Zoom H6 Handy Recorder with external microphone (henceforth H6) and compared with two alternatives accessible to potential participants at home: the Zoom meeting application (henceforth Zoom) and two lossless mobile phone applications (Awesome Voice Recorder, and Recorder; henceforth Phone). F0 was tracked accurately by all devices; however, for formant analysis (F1, F2, F3) Phone performed better than Zoom, i.e. more similarly to H6, though data extraction method (VoiceSauce, Praat) also resulted in differences. In addition, Zoom recordings exhibited unexpected drops in intensity. The results suggest that lossless format phone recordings present a viable option for at least some phonetic studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2021comparing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Jepson, Kathleen and Lohfink, Georg and Arvaniti, Amalia}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Comparing acoustic analyses of speech data collected remotely}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1121/10.0005132}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0001-4966}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Journal of the Acoustical Society of America}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3910--3916}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Acoustical Society of America}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{149}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="zhang2020segment-sing" class="col-sm-8"> <div class="title">Segment Duration and Proportion in Mandarin Singing</div> <div class="author"> <em>Cong Zhang</em>, and Xinrong Wang</div> <div class="periodical"> <em>In Proc. Speech Prosody 2020</em>, Oct 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/speechprosody_2020/zhang20c_speechprosody.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-archive.org/speechprosody_2020/zhang20c_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/ead87/?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://osf.io/ybdup?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Presentation</a> </div> <div class="abstract hidden"> <p>Speech-based singing synthesis has various merits while it also has unsolved issues. One of the most noticeable issues is the segment duration and proportion in synthesised singing, which is caused by the difference in the short syllables in speech and the lengthened syllables in singing. This study therefore investigates how syllables are lengthened in Mandarin singing data. A total of 20 songs from the MIREX singing corpus were segmented and analysed. The results showed that (1) the segment proportions in Mandarin syllables are different in speech and in singing; (2) the lengthening is influenced more by the slots in the syllable structure than by the types of segments; (3) in the syllable structure of CGVX in Mandarin, the nuclear V lengthens the most and X follows. The durations of C and G also increase but their proportions in a syllable decrease.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020segment-sing</span><span class="p">,</span>
  <span class="na">presentation</span> <span class="p">=</span> <span class="s">{https://osf.io/ybdup?view_only=c87fe156d1874ffba8a16cc363b225af}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Wang, Xinrong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Segment Duration and Proportion in Mandarin Singing}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. Speech Prosody 2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{596--600}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/SpeechProsody.2020-122}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">tone-intonation</abbr></div> <div id="zhang2019stacking" class="col-sm-8"> <div class="title">Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>In Proceeding of 19th International Congress of Phonetic Sciences</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.internationalphoneticassociation.org/icphs-proceedings/ICPhS2019/papers/ICPhS_1842.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Teasing apart lexical prosody and sentence prosody has been one of the most difficult tasks in the study of intonational tunes in tonal languages. Are different prosodic manifestations stacked, or are they an integrated whole? With evidence from production and perception data of the intonational yes/no question tune in Tianjin Mandarin at sentence level, this paper proposes that (1) lexical tonal alterations (a.k.a tone sandhi) are lexical-level prosody and do not belong to sentence-level tune; (2) pitch accents induced by information structure are “intra-tune” features, which are such sentence-level prosody features that do not cause sentence type change. Despite being sentence-level prosody features, they are not a part of the tune for intonational yes/no question.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2019stacking</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceeding of 19th International Congress of Phonetic Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language}}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{intonation,tianjin mandarin,tone}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#4A789C"><a href="">Thesis</a></abbr></div> <div id="zhang2018dphil" class="col-sm-8"> <div class="title">Tianjin Mandarin Tones and Tunes</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>Doctoral thesis: University of Oxford</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ora.ox.ac.uk/objects/uuid:3149a35c-e6c2-4f43-a41a-bdc08ebf08f6" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ora.ox.ac.uk/objects/uuid:3149a35c-e6c2-4f43-a41a-bdc08ebf08f6/files/m7641933f05b45acbbc25093eb87fe54d" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p> Lexical tones and intonational tunes are both mainly realised through pitch modulation. What role does intonation play in a language which has a lexical tonal contrast? Can one separate ‘tone’ from ‘intonation’? If yes, how do lexical tones interact with intonational tunes? In order to answer these questions, this thesis investigates how tone and intonation interact during production and perception in Tianjin Mandarin, by means of examining the components of different intonational tunes under the Autosegmental-Metrical (AM) Framework (Pierrehumbert, 1980), and the cues native listeners use during the tune identification process. Chapter 1 – 3 are the introductory chapters: Chapter 1 introduces the topic of research, and sketches the three research goals for this thesis – the theoretical goal, the documentation goal, and the methodological goal; Chapter 2 addresses the theoretical foundation of this thesis – the AM theory; and Chapter 3 outlines the linguistic background of Tianjin Mandarin. Chapter 4 presents production studies of the tune of intonational Yes/No questions (IntQ) in Tianjin Mandarin. A total of six native Tianjin speakers were recorded for monosyllabic words in isolation (Mono(ISO)) and monosyllabic words as sentence prominence (Mono(SEN)), with statement tune and IntQ tune, respectively. The results show that when a monosyllabic word is produced in isolation, the IntQ tune has a raised register, and a floating H% boundary tone at the end of the intonational phrase. When a monosyllabic word is in sentence prominence position, the IntQ tune also has a raised register, a floating H% boundary tone, as well as a H* pitch accent coming from the focus and a post-focus compression. The IntQ tune is: [H* pitch accent + (post-focus compression) + floating H̥% boundary tone] higher register. To further investigate how the IntQ tune is represented, three perception experiments were conducted on monosyllabic words in isolation, monosyllabic words as sentence prominence, and sentences with monosyllabic words as prominence in Chapter 5. A total of 28 native Tianjin Mandarin speakers participated in the experiments. They were asked to identify the tunes (yes-no question or statement) of the audio stimuli. The accuracy of their responses and reaction time together show that they strongly prefer the H-Rising lexical tone for IntQs, and L-Falling lexical tone for statements, which indicate that they look for the low register information during the identification of statements, and a H boundary tone for the IntQ tune. Another important tune, chanted call (CC) tune, was also studied to further investigate the possibilities of intonational tunes in a tonal language in Chapter 6. Six native speakers’ production of monosyllabic words and disyllabic words were recorded. The results show that there is a L% boundary tone at the end of the intonational phrase, regardless of the lexical tones. Different from the IntQ data, the L% boundary tone is phonetically manifested and overrode the lexical tone contours. A H* pitch accent was found to be associated with the H of each lexical tone. Lengthening was also found in the CC tune. The CC tune in Tianjin Mandarin can be represented as follows: [[H*]sustained]higher register + L%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">zhang2018dphil</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Doctoral thesis}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Doctoral thesis: University of Oxford}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Tianjin Mandarin Tones and Tunes}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5287/ora-jnnqaven0}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="zhang2018chanted" class="col-sm-8"> <div class="title">Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>In 9th International Conference on Speech Prosody 2018</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/speechprosody_2018/zhang18c_speechprosody.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-archive.org/speechprosody_2018/zhang18c_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>This paper examines the chanted call tune in Tianjin Mandarin in order to investigate the possibilities of intonational components, i.e. pitch accents, boundary tones, etc., in a tonal language. Six native Tianjin speakers’ production of disyllabic names and kinship terms were recorded. The speech materials were composed of a set of left-prominent disyllabic names and a set of right-prominent disyllabic names. The results show that there is a L% boundary tone at the end of the intonational phrase, regardless of the lexical tones. Different from the IntQ data, the L% boundary tone is phonetically manifested and overrode the lexical tone contours. A H* pitch accent was found to be associated with the H of each lexical tone. Lengthening was also found in the CC tune. The CC tune in Tianjin Mandarin can be represented as follows: [[H*]sustained]higher register + L%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2018chanted</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{9th International Conference on Speech Prosody 2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/SpeechProsody.2018-106}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{522--526}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ISCA}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">fluency</abbr></div> <div id="wright2015disfluency" class="col-sm-8"> <div class="title">The Effect of Study Abroad Experience on L2 Mandarin Disfluency in Different Types of Tasks</div> <div class="author"> Clare Wright, and <em>Cong Zhang</em> </div> <div class="periodical"> <em>In Proceeding of The Disfluency in Spontaneous Speech</em>, Oct 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://shorturl.at/cpsST" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Disfluency is a common phenomenon in L2 speech, especially in beginners’ speech. Whether studying abroad can help with reducing their disfluency or not remains debated. We examined longitudinal data from 10 adult English instructed learners of Mandarin measured before and after ten months of studying abroad (SA) in this paper. We used two speaking tasks comparing pre-planned vs. unplanned spontaneous speech to compare differences over time and between tasks, using eight linguistic and temporal fluency measures (analysed using CLAN and PRAAT). Overall mean linguistic and temporal fluency scores improved significantly (p &lt; .05), especially speech rate (p &lt;.01), supporting the general claim that SA favours oral development, particularly fluency. Further analysis revealed task differences at both times of measurement, but with greater improvement in the spontaneous task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wright2015disfluency</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wright, Clare and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The Effect of Study Abroad Experience on L2 Mandarin Disfluency in Different Types of Tasks}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceeding of The Disfluency in Spontaneous Speech}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Lickley, Robin}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{fluency,l2 mandarin,study abroad}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">fluency</abbr></div> <div id="wright2014fluency" class="col-sm-8"> <div class="title">Examining the Effects of Study Abroad on Mandarin Chinese Language Development among UK University Learners</div> <div class="author"> Clare Wright, and <em>Cong Zhang</em> </div> <div class="periodical"> <em>Newcastle Working Papers in Linguistics</em>, Oct 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://centaur.reading.ac.uk/37687/1/NWPL%20pre-publication.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>This study tracked ten third-year English students learning Mandarin Chinese as a second language (L2) at a UK university, to examine changes in L2 Mandarin during an eight-month period spent studying abroad (SA). We used three writing tasks and four speaking tasks as measures of writing and speaking proficiency, to assess total output, grammatical accuracy, lexical development, pronunciation and fluency, repeated before and after SA in China. Overall mean oral proficiency scores improved significantly (p &lt; .05), especially speech rate (p &lt;.01), supporting the claim that SA favours fluency development (Freed et al. 2004), although the measures highlighted difficulties in clarifying precisely how to assess oral proficiency. Written proficiency showed fewer marked improvements: only one writing test (an untimed short essay) significantly improved in length (p &lt;.05), and increased complex grammar (use of de-relative clause morphemes, p &lt;.001). A sub-group (n=7) provided quantitative data on L2 Mandarin use at different times during SA, showing clear individual differences, highlighting the value of capturing details of students’ experiences during SA (Regan et al. 2009). We also note the lack of standardised linguistically-informed measures for tracking development in L2 Mandarin (Freed et al. 2004; Pallotti 2009; De Jong et al. 2012). Further research is therefore much needed to identify systematic linguistic development in L2 Mandarin, and also to bridge theory and practice in L2 Mandarin language teaching to clarify the interconnecting factors that affect L2 Mandarin language development.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wright2014fluency</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wright, Clare and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Examining the Effects of Study Abroad on Mandarin Chinese Language Development among UK University Learners}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Newcastle Working Papers in Linguistics}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{fluency}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{67--84}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">feedback</abbr></div> <div id="zhang2014feedback" class="col-sm-8"> <div class="title">The effect of immediate feedback on the perception of Mandarin lexical tones by non-native speakers of Mandarin</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>St. Anne’s Annual Review</em>, Oct 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://st-annes-mcr.org.uk/staar/publications/staar-5-2014/zhang-2014-the-perception-of-mandarin-lexical-tones-by-non-native-speakers/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://centaur.reading.ac.uk/37687/1/NWPL%20pre-publication.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p> Lexical tone is one of the most difficult issues in learning Mandarin as a foreign language. Various efforts have been made by training non-native speakers to improve the perception of Mandarin lexical tones. Immediate feedback, as an essential and efficient way of perceptual learning, however, has been understudied. An AX discrimination task is used to test whether the participants’ perception of Mandarin lexical tones improves after being given immediate feedback. The result shows an evident effect of immediate feedback on the perception of Mandarin lexical tones, both within the experiment groups as well as between the experiment group and the control group</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2014feedback</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{St. Anne's Annual Review}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105--125}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The effect of immediate feedback on the perception of Mandarin lexical tones by non-native speakers of Mandarin}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#4A789C"><a href="">Thesis</a></abbr></div> <div id="zhang2012MA" class="col-sm-8"> <div class="title">The effect of immediate and simple feedback on the perception of Mandarin lexical tones by English speakers</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>MA dissertation: Newcastle University, UK</em>, Oct 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Zhang_2012_MA%20Dissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Lexical tone is an important feature in Mandarin. However, it caused substantial difficulty for the non-Mandarin speakers. Since lexical tones are not completely illegible to non-Mandarin speakers, many methods are employed to improve the perception of Mandarin lexical tones by non-Mandarin speakers. One method is perceptual learning. Perceptual learning is a learning style through which people pick up “previously unused information” (Gibson &amp; Gibson1955). Most perceptual learning studies on Mandarin lexical tone perception have used perceptual training as the condition for the participants to pick up information (e.g. Wang et al. 1999; Wang et al. 2003; Francis et al. 2008). Feedback, which is another important tool for perceptual learning, is nevertheless understudied in Mandarin lexical tone perceptual learning studies. Since feedback has many categories, in the current study, only the immediate and simplest form of feedback is examined. Whether the perception of the lexical tones by non-Mandarin speakers can be improved by receiving immediate and simple feedback is the focus of this study. The aims of this study are: 1) to investigate the effect of feedback and provide future studies with experimental grounds on the use of feedback; 2) to suggest the use of feedback in Computer Assisted Language Learning through investigating the effect of feedback; 3) to contribute to the existing theories, such as Autosegmental Theory (Goldsmith 1979), Categorical Perception (Best 1995), Noticing Hypothesis (Schmidt 1990), with empirical evidence. The current study examined the perception of Mandarin lexical tones by 24 native British English speakers, with 5 native Mandarin speakers as one of the control groups. The experiment made use of an AX discrimination task which required the participants to make judgments on whether the tones of 160 pairs of stimuli were the same or not, regardless of the consonants or vowels. The experiment group consisted of 12 native British English speakers. This group received simple feedback which only indicated the incorrectness on the incorrect judgments immediately after the judgment was made. As a control group, the other 12 participants did not receive any feedback. The results of the experiment showed that simple immediate feedback did not have a significant effect on the perception of Mandarin lexical tones by English speakers, both in terms of accuracy and reaction time. The reasons for the results are mainly discussed in terms of feedback types, individual differences on perceptual learning, and the influence from musical experience on lexical tone perception. The first chapter of this dissertation is a general introduction. In the second chapter, some background information about the phonology of Mandarin syllables is introduced. The third chapter reviews the previous studies on lexical tone perception, perceptual learning and feedback, to establish the basis for this study. A pilot experiment and a full-scale experiment are reported in the fourth and the fifth chapter respectively. The last chapter is dedicated to the general discussion, conclusion and suggestions for future studies. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">zhang2012MA</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The effect of immediate and simple feedback on the perception of Mandarin lexical tones by English speakers}}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{MA dissertation: Newcastle University, UK}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h1>Preprints</h1> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">featureTTS</abbr></div> <div id="zhang2021applying" class="col-sm-8"> <div class="title">Applying Phonological Features in Multilingual Text-To-Speech</div> <div class="author"> <em>Cong Zhang</em>, Huinan Zeng, Huang Liu, and Jiewen Zheng</div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/ftp/arxiv/papers/2110/2110.03609.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://congzhang365.github.io/feature_tts/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Presentation</a> </div> <div class="abstract hidden"> <p>This study investigates whether phonological features can be applied in text-to-speech systems to generate native and non-native speech. We present a mapping between ARPABET/pinyin-&gt;SAMPA/SAMPA-SC-&gt;phonological features in this paper, and tested whether native, non-native, and code-switched speech could be successfully generated using this mapping. We ran two experiments, one with a small dataset and one with a larger dataset. The results proved that phonological features can be a feasible input system, although it needs further investigation to improve model performance. The accented output generated by the TTS models also helps with understanding human second language acquisition processes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2021applying</span><span class="p">,</span>
  <span class="na">presentation</span> <span class="p">=</span> <span class="s">{https://congzhang365.github.io/feature_tts/}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Applying Phonological Features in Multilingual Text-To-Speech}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Zeng, Huinan and Liu, Huang and Zheng, Jiewen}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{journal}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">intonation</abbr></div> <div id="zhang2021floating" class="col-sm-8"> <div class="title">Floating Boundary Tone: Production and Perception of Syntactically Unmarked Polar Question in Tianjin Mandarin</div> <div class="author"> <em>Cong Zhang</em>, and Aditi Lahiri</div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://psyarxiv.com/rd7gw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://psyarxiv.com/rd7gw/download" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The present study investigates the intonational tune of syntactically unmarked polar question in Tianjin Mandarin. A production study was conducted to examine the phonological features of the syntactically unmarked polar question (a.k.a intonational yes/no question) tune by comparing against the statement tune. The results show a significant register lift HR and a high floating boundary tone H̥I. The tone shape and tone register played a significant role in how the tunes vary. A tune identification task then further verifies whether the two prosodic features concluded from the production are used in perception. The results showed that both the register difference and the boundary tone made a difference in native speakers’ perception in discriminating questions from statements.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2021floating</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.31234/osf.io/rd7gw}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Floating Boundary Tone: Production and Perception of Syntactically Unmarked Polar Question in Tianjin Mandarin}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Lahiri, Aditi}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Cong Zhang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JEZS4MT5D"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5JEZS4MT5D");</script> </body> </html>