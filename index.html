<!DOCTYPE html> <html lang="en"> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JEZS4MT5D"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5JEZS4MT5D");</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Dr. Cong Zhang</title> <meta name="author" content="Cong Zhang"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="cong, cong zhang, newcaslte, oxford, linguistics, prosody, phonetics, phonology, speech"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icon.jpg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://congzhang-linguist.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">awards</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Dr. Cong Zhang </h1> <p class="desc">cong.zhang at newcastle.ac.uk</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Âº†ËÅ™</p> <p>Lecturer in Phonetics and Phonology</p> <p>Newcastle University</p> </div> </div> <div class="clearfix"> <p>Hi! I‚Äôm Cong Zhang ([ts ∞ ä≈ã t É…ë≈ã], each with a high-level tone (tone number 55, <a href="https://congzhang-linguist.github.io/blog/2023/my-name/">more about my name</a>). I am a Lecturer in Phonetics and Phonology at the <a href="http://ncl.ac.uk/ecls/" target="_blank" rel="noopener noreferrer">School of Education, Communication and Language Sciences</a>, Newcastle University, UK.</p> <p>My research mainly focuses on aspects of speech prosody (e.g. intonation, lexical tone, rhythm), using a variety of approaches including:</p> <ul> <li>Phonetics and Phonology (Laboratory Phonology)</li> <li>Psycholinguistics</li> <li>Computational linguistics</li> <li>Language Acquisition</li> </ul> <p>I received a DPhil degree from the <a href="http://brainlab.clp.ox.ac.uk/" target="_blank" rel="noopener noreferrer">Language and Brain Lab</a>, University of Oxford. My DPhil thesis, supervised by Professor Aditi Lahiri, was about the intonational tunes in a tonal language ‚Äî Tianjin Mandarin (<a href="https://ora.ox.ac.uk/objects/uuid:3149a35c-e6c2-4f43-a41a-bdc08ebf08f6" target="_blank" rel="noopener noreferrer">More about my DPhil project</a>).</p> <p>I did my Master‚Äôs in Linguistics and Language Acquisition from the <a href="https://www.ncl.ac.uk/elll/" target="_blank" rel="noopener noreferrer">School of English Literature, Language and Linguistics</a>, Newcastle University (UK). There, I worked on a number of projects including child language acquisition, second language acquisition, Mandarin lexical tone perception, etc.</p> <p>For my undergraduate degree, I studied Translation and Interpreting at <a href="https://www.bfsu.edu.cn/" target="_blank" rel="noopener noreferrer">Beijing Foreign Studies University</a> (China). I am therefore also interested in studies about translation and interpreting.</p> <p>Following my DPhil, I worked as a TTS linguist (Linguistics Engineer) in A-Lab at Rokid Inc. One of my major projects was Singing Synthesis (text-to-singing). After this, I came back to academia and worked on the ERC project <a href="https://sprintproject.io/" target="_blank" rel="noopener noreferrer">SPRINT</a> (i.e. Speech Prosody in Interaction: The form and function of intonation in human communication, ERC-ADG-835263 ) and further looked into the aspects of English and Greek intonation in speech production.</p> <div class="cv"> </div> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">May 1, 2025</th> <td> We are hosting a satelite workshop <strong><a href="https://agenda.uib.es/120122/section/53647/6th-phonetics-and-phonology-in-europe-pape-2025.html" target="_blank" rel="noopener noreferrer">Prosody in Languages of the Middle East</a></strong> at <a href="https://agenda.uib.es/120122/detail/6th-phonetics-and-phonology-in-europe-pape-2025.html" target="_blank" rel="noopener noreferrer">PaPE 2025</a>. We will also have a special issue for this workshop soon. Stay tuned! </td> </tr> <tr> <th scope="row">Mar 27, 2025</th> <td> Invited talk at HKPU: <strong><a href="https://www.polyu.edu.hk/engl/news-and-events/events/2025/3/seminar-i-tonal-tug-of-war---the-interplay-of-lexical-and-sentence-prosody" target="_blank" rel="noopener noreferrer">Tonal Tug-of-War: The Interplay of Lexical and Sentence Prosody</a></strong>. Recording available <a href="https://youtu.be/MbDx-Toe6NY?si=5Upb9qcJRYKa46mL" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Nov 10, 2024</th> <td> Our Festival of Social Science event was reported by <a href="https://www.bbc.co.uk/news/articles/crezw2zx138o" target="_blank" rel="noopener noreferrer">BBC</a>, <a href="https://www.mirror.co.uk/news/uk-news/geordie-accent-studied-newcastle-university-34004194" target="_blank" rel="noopener noreferrer">Mirror</a>, and <a href="https://www.chroniclelive.co.uk/news/north-east-news/wey-aye-man-newcastle-university-30246657" target="_blank" rel="noopener noreferrer">Chronicle</a>! Glad to see many people loving our event too! Check out our project website <a href="https://research.ncl.ac.uk/ne-accent-games/" target="_blank" rel="noopener noreferrer">here</a>. </td> </tr> <tr> <th scope="row">Oct 2, 2024</th> <td> üìë New paper: Xu, C. &amp; Zhang, C. (2024) ‚ÄúA cross-linguistic review of citation tone production studies: Methodology and recommendations‚Äù. J. Acoust. Soc. Am. 1 October 2024; 156 (4): 2538‚Äì2565. <a href="https://doi.org/10.1121/10.0032356" target="_blank" rel="noopener noreferrer">https://doi.org/10.1121/10.0032356</a> </td> </tr> <tr> <th scope="row">Oct 1, 2024</th> <td> üìë New paper: Zhang, C., Jepson, K. &amp; Chuang, Y., (2024) ‚ÄúInvestigating differences in lab-quality and remote recording methods with dynamic acoustic measures‚Äù, Laboratory Phonology 15(1). doi: <a href="https://doi.org/10.16995/labphon.10492" target="_blank" rel="noopener noreferrer">https://doi.org/10.16995/labphon.10492</a> </td> </tr> <tr> <th scope="row">Aug 10, 2024</th> <td> üèÜ Our knowledge exchange event <strong><a href="https://festivalofsocialscience.com/events/wey-aye-man/" target="_blank" rel="noopener noreferrer">Wey aye, man - Think you know a North-East accent when you hear one?</a></strong> has been funded by ESRC to take part in <a href="https://www.ukri.org/what-we-do/public-engagement/public-engagement-esrc/festival-of-social-science/" target="_blank" rel="noopener noreferrer">Festival of Social Sciences</a> on 2nd November at <a href="https://www.farrellcentre.org.uk/" target="_blank" rel="noopener noreferrer">Farrell Centre</a>! </td> </tr> <tr> <th scope="row">Jun 19, 2024</th> <td> üèÜ I have just won the <strong>Second Prize</strong> of Newcastle University‚Äôs Open Research Awards! </td> </tr> <tr> <th scope="row">Mar 27, 2024</th> <td> üìë New paper: Zhang, C., Jepson, K., &amp; Chuang, Y. (in press). Investigating differences in lab-quality and remote recording methods with dynamic acoustic measures. Laboratory Phonology. Preprint: <a href="https://congzhang-linguist.github.io/assets/pdf/ZhangEtAL_2024_LabPhon.pdf">(Link)</a> </td> </tr> </table> <center><a href="./news/">[News archive]</a></center> </div> </div> <div class="cv"> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Education</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2018 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">DPhil</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">University of Oxford, UK</h6> <ul class="items"> <li> <span class="item">General Linguistics and Comparative Philology</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2012 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">M.A.(distinction)</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Newcastle Univeristy, UK</h6> <ul class="items"> <li> <span class="item">Linguistics &amp; Language acquisition</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2011 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">B.A. (with Excellent Graduate Award)</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Beijing Foreign studies University, China</h6> <ul class="items"> <li> <span class="item">English Language and Literature (Translation and Interpreting)</span> </li> </ul> </div> </div> </li> </ul> </div> </div> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Employement</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2022- </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">Lecturer in Phonetics and Phonology</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Newcastle University, UK</h6> <ul class="items"> <li> <span class="item">School of Education, Communication and Language Sciences</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2019-2022 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">Postdoctoral Research Associate</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">University of Kent, UK (2019-2020) &amp; Radboud University, Netherlands (2020-2022)</h6> <ul class="items"> <li> <span class="item">Speech Prosody in Interaction -- The form and function of intonation in human communication (ERC-ADG-835263)</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2018-2019 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">Text-to-Speech Linguist / Linguistics Engineer</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Algorithm-Lab, Rokid Inc. (Beijing, China)</h6> </div> </div> </li> </ul> </div> </div> </div> <div class="publications"> <h2>Selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">tone</abbr></div> <div id="xu2024across" class="col-sm-8"> <div class="title">A cross-linguistic review of citation tone production studies: Methodology and recommendations</div> <div class="author"> Chenzi Xu,¬†and¬†<em>Cong Zhang</em> </div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1121/10.0032356" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://pubs.aip.org/asa/jasa/article-pdf/156/4/2538/20213761/2538_1_10.0032356.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The study of citation tones, lexical tones produced in isolation, is one of the first steps towards understanding speech prosody in tone languages. However, methodologies for investigating citation tones vary significantly, often leading to limited comparability of tone inventories, both within and across languages. This paper presents a systematic review of research methods and practices in 136 citation tone studies on 129 tonal language varieties in China, including 99 studies published in Chinese, which are therefore not easily available to an international scientific readership. The review provides an overview of possible analytical decisions along the research pipeline, and unveils considerable variation in data collection, analysis, and reporting conventions, particularly in how f0, the primary acoustic correlate for tone, is operationalised and reported across studies. Key methodological issues are identified, including small sample sizes and inadequate transparency in communicating methodological decisions and procedure. This paper offers a clear road map for citation tone production research and proposes a range of recommendations on speaker sampling, experimental design, acoustic processing techniques, f0 analysis, and result reporting, with the goal of facilitating future tonal research and enhancing resources for underrepresented tonal varieties.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">remote</abbr></div> <div id="zhang2024investigating" class="col-sm-8"> <div class="title">Investigating differences in lab-quality and remote recording methods with dynamic acoustic measures</div> <div class="author"> <em>Cong Zhang</em>,¬†Kathleen Jepson,¬†and¬†Yu-Ying Chuang</div> <div class="periodical"> <em>Laboratory Phonology</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.journal-labphon.org/article/10492/galley/33869/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.journal-labphon.org/article/10492/galley/33869/download/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Increasingly, phonetic research uses data collected from participants who record themselves on readily available devices. Though such recordings are convenient, their suitability for acoustic analysis remains an open question, especially regarding how recording methods affect acoustic measures over time. We used Quantile Generalized Additive Mixed Models (QGAMMs) to analyze measures of F0, intensity, and the first and second formants, comparing files recorded using a laboratory-standard recording method (Zoom H6 recorder with an external microphone), to three remote recording methods: (1) the Awesome Voice Recorder application on a smartphone (AVR), (2) the Zoom meeting application with default settings (Zoom-default), and (3) the Zoom meeting application with the ‚ÄúTurn on Original Sound‚Äù setting (Zoom-raw). A linear temporal alignment issue was observed for the Zoom methods over the course of the long, recording session files; however, the difference was not significant for utterance-length files. F0 was reliably measured using all methods. Intensity and formants presented non-linear differences across methods that could not be corrected for simply. Overall, the AVR files were most similar to the H6‚Äôs, and so AVR is deemed to be a more reliable recording method than either Zoom-default or Zoom-raw.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2024investigating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Investigating differences in lab-quality and remote recording methods with dynamic acoustic measures}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Jepson, Kathleen and Chuang, Yu-Ying}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Laboratory Phonology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.16995/labphon.10492}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">aphasia</abbr></div> <div id="zhang2024aphasia" class="col-sm-8"> <div class="title">Prosody of speech production in latent post-stroke aphasia </div> <div class="author"> <em>Cong Zhang</em>,¬†Tong Li,¬†Gayle DeDe,¬†and¬†Christos Salis</div> <div class="periodical"> <em>In Interspeech 2024</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ZhangEtAl_2024_Interspeech.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This study explores prosodic production in latent aphasia, a mild form of aphasia associated with left-hemisphere brain damage (e.g. stroke). Unlike prior research on moderate to severe aphasia, we investigated latent aphasia, which can seem to have very similar speech production with neurotypical speech. We analysed the f0, intensity and duration of utterance-initial and utterance-final words of ten speakers with latent aphasia and ten matching controls. Regression models were fitted to improve our understanding of this understudied type of very mild aphasia. The results highlighted varying degrees of differences in all three prosodic measures between groups. We also investigated the diagnostic classification of latent aphasia versus neurotypical control using random forest, aiming to build a fast and reliable tool to assist with the identification of latent aphasia. The random forest analysis also reinforced the significance of prosodic features in distinguishing latent aphasia.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024aphasia</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prosody of speech production in latent post-stroke aphasia }</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Li, Tong and DeDe, Gayle and Salis, Christos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">gamification</abbr></div> <div id="kim2024collecting" class="col-sm-8"> <div class="title">Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics</div> <div class="author"> Yoolim Kim,¬†Vita V Kogan,¬†and¬†<em>Cong Zhang</em> </div> <div class="periodical"> <em>Applied Linguistics</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1093/applin/amad039" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://academic.oup.com/applij/advance-article/doi/10.1093/applin/amad039/7223350?utm_source=authortollfreelink&amp;utm_campaign=applij&amp;utm_medium=email&amp;guestAccessKey=6ebc0401-70dd-4539-982e-25e6608e3a34&amp;login=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Gamification of behavioral experiments has been applied successfully to research in a number of disciplines, including linguistics. We believe that these methods have been underutilized in applied linguistics, in particular second-language acquisition research. The incorporation of games and gaming elements (gamification) in behavioral experiments has been shown to mitigate many of the practical constraints characteristic of lab settings, such as limited recruitment or only achieving small-scale data. However, such constraints are no longer an issue with gamified and game-based experiments, and as a result, data collection can occur remotely with greater ease and on a much wider scale, yielding data that are ecologically valid and robust. These methods enable the collection of data that are comparable in quality to the data collected in more traditional settings while engaging far more diverse participants with different language backgrounds that are more representative of the greater population. We highlight three successful applications of using games and gamification with applied linguistic experiments to illustrate the effectiveness of such approaches in a greater effort to invite other applied linguists to do the same.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2024collecting</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Yoolim and Kogan, Vita V and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Linguistics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{198-205}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0142-6001}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/applin/amad039}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">BigTeam</abbr></div> <div id="coretta2023multidimensional" class="col-sm-8"> <div class="title">Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses</div> <div class="author"> Ste Coretta,¬†Joseph V Casillas,¬† ...,¬†<em>Cong Zhang</em>,¬† ...,¬†and¬†Timo B Roettger</div> <div class="periodical"> <em>Advances in Methods and Practices in Psychological Sciences</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1177/25152459231162567" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://journals.sagepub.com/doi/pdf/10.1177/25152459231162567?download=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis which can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling, but also from decisions regarding the quantification of the measured behavior. In the present study, we gave the same speech production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further find little to no evidence that the observed variability can be explained by analysts‚Äô prior beliefs, expertise or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system and calibrate their (un)certainty in their conclusions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">coretta2023multidimensional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Coretta, Ste and Casillas, Joseph V and ... and Zhang, Cong and ... and Roettger, Timo B}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Methods and Practices in Psychological Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICPhS</abbr></div> <div id="zhang2023language-redundancy" class="col-sm-8"> <div class="title">Language redundancy effects on F0: A preliminary controlled study</div> <div class="author"> <em>Cong Zhang</em>,¬†Catherine Lai,¬†Ricardo Souza,¬†Alice Turk,¬†and¬†Tina B√∂gel</div> <div class="periodical"> <em>In Proceeding of 20th International Congress of Phonetic Sciences</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="html" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/ZhangEtAl_2023_ICPhS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/ZhangEtAl_2023_ICPhS_poster.pdf" class="btn btn-sm z-depth-0" role="button">Presentation</a> </div> <div class="abstract hidden"> <p>Previous research suggests that words with a high level of language redundancy (i.e. recognition likelihood from familiarity and predictability based on syntactic, pragmatic, and semantic factors) have reduced acoustic salience, such as shorter duration and reduced vowels. The Smooth Signal Redundancy Hypothesis proposes that acoustic salience is controlled via prosodic structure, and makes the prediction that parameters such as fundamental frequency should also be affected by language redundancy. This study investigates the relationship of F0 with lexical frequency, together with bigram (verb-adjective or adjective-noun) frequency and the ratio between these two bigram frequencies. Results from a carefully controlled experiment with quadruplets of minimal pairs suggests that language redundancy can affect fundamental frequency in English.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023language-redundancy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Language redundancy effects on F0: A preliminary controlled study}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Lai, Catherine and Napole√£o de Souza, Ricardo and Turk, Alice and B√∂gel, Tina}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceeding of 20th International Congress of Phonetic Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP Findings</abbr></div> <div id="zhu2022bootstrapping" class="col-sm-8"> <div class="title">Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings</div> <div class="author"> Jian Zhu,¬†Zuoyu Tian,¬†Yadong Liu,¬†<em>Cong Zhang</em>,¬†and¬†Chia-wen Lo</div> <div class="periodical"> <em>Findings of Empirical Methods in Natural Language Processing</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.findings-emnlp.81/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://aclanthology.org/2022.findings-emnlp.81.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/spoken_sent_embedding" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5¬†0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2022bootstrapping</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Jian and Tian, Zuoyu and Liu, Yadong and Zhang, Cong and Lo, Chia-wen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Findings of Empirical Methods in Natural Language Processing}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">rhythm</abbr></div> <div id="sun2022task" class="col-sm-8"> <div class="title">Task effect on L2 rhythm production by Cantonese learners of Portuguese</div> <div class="author"> Yuqi Sun,¬†and¬†<em>Cong Zhang</em> </div> <div class="periodical"> <em>DELTA: Documenta√ß√£o de Estudos em Ling√º√≠stica Te√≥rica e Aplicada</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1590/1678-460X202258943" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.scielo.br/j/delta/a/QwGKgDkWkvJbNsZ9CrgZNvb/?format=pdf&amp;lang=en" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/spoken_sent_embedding" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This study examines L2 Portuguese speech produced by eight native Cantonese speakers from Macao, China. The aims of this study are to investigate (1) whether the speech rhythm in L2 Portuguese is more source-like (more similar to Cantonese) or more target-like (more similar to Portuguese), and (2) whether L2 speech rhythm differs across three different tasks: a reading task, a retelling task, and an interpreting task. Seven rhythm metrics, i.e., %V, ŒîC, ŒîV, VarcoC, VarcoV, rPVI_C, and nPVI_V, were adopted for comparison and investigation. The results showed that L2 Portuguese rhythm produced by Cantonese speakers differed from L1 Portuguese speakers‚Äô rhythm. R-deletion and vowel epenthesis were the reasons for the variabilities and instabilities of L2 Portuguese production by Cantonese learners, as they affect the duration and the number of vowel intervals and consonantal intervals. Moreover, in Cantonese learners‚Äô L2 Portuguese production, the semi-spontaneous tasks (retelling and interpreting) presented a significant difference from the reading task. The driving force for such a difference was the cognitive load behind the tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sun2022task</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sun, Yuqi and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task effect on L2 rhythm production by Cantonese learners of Portuguese}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0102-4450}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1590/1678-460X202258943}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{DELTA: Documenta√ß√£o de Estudos em Ling√º√≠stica Te√≥rica e Aplicada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pontif√≠cia Universidade Cat√≥lica de S√£o Paulo - PUC-SP}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{202258943}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="zhu2022byt5-g2p" class="col-sm-8"> <div class="title">ByT5 model for massively multilingual grapheme-to-phoneme conversion</div> <div class="author"> Jian Zhu*, Cong Zhang*,¬†and¬†David Jurgens [* equal contribution]</div> <div class="periodical"> <em>In Interspeech 2022</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.03067.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/CharsiuG2P" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2022byt5-g2p</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ByT5 model for massively multilingual grapheme-to-phoneme conversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{Jian Zhu*, Cong Zhang*} and {[* equal contribution]}, David Jurgens}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="zhang2021synchronising" class="col-sm-8"> <div class="title">Synchronising Speech Segments with Musical Beats in Mandarin and English Singing</div> <div class="author"> <em>Cong Zhang</em>,¬†and¬†Jian Zhu</div> <div class="periodical"> <em>In Interspeech 2021</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21i_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-archive.org/interspeech_2021/zhang21i_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/8m5bj/?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Generating synthesised singing voice with models trained on speech data has many advantages due to the models‚Äô flexibility and controllability. However, since the information about the temporal relationship between segments and beats are lacking in speech training data, the synthesised singing may sound off-beat at times. Therefore, the availability of the information on the temporal relationship between speech segments and music beats is crucial. The current study investigated the segment-beat synchronisation in singing data, with hypotheses formed based on the linguistics theories of P-centre and sonority hierarchy. A Mandarin corpus and an English corpus of professional singing data were manually annotated and analysed. The results showed that the presence of musical beats was more dependent on segment duration than sonority. However, the sonority hierarchy and the P-centre theory were highly related to the location of beats. Mandarin and English demonstrated cross-linguistic variations despite exhibiting common patterns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2021synchronising</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Zhu, Jian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Synchronising Speech Segments with Musical Beats in Mandarin and English Singing}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1199--1203}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2021-1841}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JASA</abbr></div> <div id="zhang2021comparing" class="col-sm-8"> <div class="title">Comparing acoustic analyses of speech data collected remotely</div> <div class="author"> <em>Cong Zhang</em>,¬†Kathleen Jepson,¬†Georg Lohfink,¬†and¬†Amalia Arvaniti</div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pubs.aip.org/asa/jasa/article/149/6/3910/1059288/Comparing-acoustic-analyses-of-speech-data" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8269758/pdf/JASMAN-000149-003910_1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Face-to-face speech data collection has been next to impossible globally due to COVID-19 restrictions. To address this problem, simultaneous recordings of three repetitions of the cardinal vowels were made using a Zoom H6 Handy Recorder with external microphone (henceforth H6) and compared with two alternatives accessible to potential participants at home: the Zoom meeting application (henceforth Zoom) and two lossless mobile phone applications (Awesome Voice Recorder, and Recorder; henceforth Phone). F0 was tracked accurately by all devices; however, for formant analysis (F1, F2, F3) Phone performed better than Zoom, i.e. more similarly to H6, though data extraction method (VoiceSauce, Praat) also resulted in differences. In addition, Zoom recordings exhibited unexpected drops in intensity. The results suggest that lossless format phone recordings present a viable option for at least some phonetic studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2021comparing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Jepson, Kathleen and Lohfink, Georg and Arvaniti, Amalia}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Comparing acoustic analyses of speech data collected remotely}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1121/10.0005132}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0001-4966}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Journal of the Acoustical Society of America}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3910--3916}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Acoustical Society of America}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{149}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="zhang2020segment-sing" class="col-sm-8"> <div class="title">Segment Duration and Proportion in Mandarin Singing</div> <div class="author"> <em>Cong Zhang</em>,¬†and¬†Xinrong Wang</div> <div class="periodical"> <em>In Proc. Speech Prosody 2020</em>, Oct 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/speechprosody_2020/zhang20c_speechprosody.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-archive.org/speechprosody_2020/zhang20c_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/ead87/?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://osf.io/ybdup?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Presentation</a> </div> <div class="abstract hidden"> <p>Speech-based singing synthesis has various merits while it also has unsolved issues. One of the most noticeable issues is the segment duration and proportion in synthesised singing, which is caused by the difference in the short syllables in speech and the lengthened syllables in singing. This study therefore investigates how syllables are lengthened in Mandarin singing data. A total of 20 songs from the MIREX singing corpus were segmented and analysed. The results showed that (1) the segment proportions in Mandarin syllables are different in speech and in singing; (2) the lengthening is influenced more by the slots in the syllable structure than by the types of segments; (3) in the syllable structure of CGVX in Mandarin, the nuclear V lengthens the most and X follows. The durations of C and G also increase but their proportions in a syllable decrease.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020segment-sing</span><span class="p">,</span>
  <span class="na">presentation</span> <span class="p">=</span> <span class="s">{https://osf.io/ybdup?view_only=c87fe156d1874ffba8a16cc363b225af}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Wang, Xinrong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Segment Duration and Proportion in Mandarin Singing}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. Speech Prosody 2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{596--600}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/SpeechProsody.2020-122}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">tone-intonation</abbr></div> <div id="zhang2019stacking" class="col-sm-8"> <div class="title">Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>In Proceeding of 19th International Congress of Phonetic Sciences</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.internationalphoneticassociation.org/icphs-proceedings/ICPhS2019/papers/ICPhS_1842.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Teasing apart lexical prosody and sentence prosody has been one of the most difficult tasks in the study of intonational tunes in tonal languages. Are different prosodic manifestations stacked, or are they an integrated whole? With evidence from production and perception data of the intonational yes/no question tune in Tianjin Mandarin at sentence level, this paper proposes that (1) lexical tonal alterations (a.k.a tone sandhi) are lexical-level prosody and do not belong to sentence-level tune; (2) pitch accents induced by information structure are ‚Äúintra-tune‚Äù features, which are such sentence-level prosody features that do not cause sentence type change. Despite being sentence-level prosody features, they are not a part of the tune for intonational yes/no question.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2019stacking</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceeding of 19th International Congress of Phonetic Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language}}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{intonation,tianjin mandarin,tone}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="zhang2018chanted" class="col-sm-8"> <div class="title">Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>In 9th International Conference on Speech Prosody 2018</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/speechprosody_2018/zhang18c_speechprosody.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-archive.org/speechprosody_2018/zhang18c_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>This paper examines the chanted call tune in Tianjin Mandarin in order to investigate the possibilities of intonational components, i.e. pitch accents, boundary tones, etc., in a tonal language. Six native Tianjin speakers‚Äô production of disyllabic names and kinship terms were recorded. The speech materials were composed of a set of left-prominent disyllabic names and a set of right-prominent disyllabic names. The results show that there is a L% boundary tone at the end of the intonational phrase, regardless of the lexical tones. Different from the IntQ data, the L% boundary tone is phonetically manifested and overrode the lexical tone contours. A H* pitch accent was found to be associated with the H of each lexical tone. Lengthening was also found in the CC tune. The CC tune in Tianjin Mandarin can be represented as follows: [[H*]sustained]higher register + L%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2018chanted</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{9th International Conference on Speech Prosody 2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/SpeechProsody.2018-106}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{522--526}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ISCA}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%6F%6E%67.%7A%68%61%6E%67@%6E%65%77%63%61%73%74%6C%65.%61%63.%75%6B" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-2561-2113" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=J9ofRc0AAAAJ&amp;hl" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/congzhang365" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://twitter.com/congprosody" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Cong Zhang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JEZS4MT5D"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5JEZS4MT5D");</script> </body> </html>