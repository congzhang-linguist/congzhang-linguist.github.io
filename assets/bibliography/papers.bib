# ------------------- 2023 ----------------


@article{coretta2023multidimensional,
    selected={False},
    bibtex_show={true},
    abbr={AMPPS}, 
    html={https://doi.org/10.1177/25152459231162567},
    pdf={https://journals.sagepub.com/doi/reader/10.1177/25152459231162567/},

    title={Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses},
    author={Coretta, Ste and Casillas, Joseph V and ... and Zhang, Cong and ... and Roettger, Timo B},
    journal={Advances in Methods and Practices in Psychological Sciences},
    year={2023},

    abstract={Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis which can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling, but also from decisions regarding the quantification of the measured behavior. In the present study, we gave the same speech production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further find little to no evidence that the observed variability can be explained by analysts' prior beliefs, expertise or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system and calibrate their (un)certainty in their conclusions.},
}

# ------------------- 2022 ----------------
@article{zhu2022listening,
    selected={True},
    bibtex_show={true},
    abbr={EMNLP Findings}, 
    pdf={https://aclanthology.org/2022.findings-emnlp.81.pdf},
    html={https://aclanthology.org/2022.findings-emnlp.81/},
    code={https://github.com/lingjzhu/spoken_sent_embedding},

    title = {Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings},
    author = {Zhu, Jian  and Tian, Zuoyu and Liu, Yadong and Zhang, Cong and Lo, Chia-wen},
    year = {2022},

    journal = {Findings of Empirical Methods in Natural Language Processing},

    abstract={Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5~0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search.}
   }


@misc{Zhang2022rhythm,
    selected={false},
    bibtex_show={true},
    abbr={R package}, 
    pdf = {https://osf.io/q6edh/download},
    html = {10.31219/osf.io/kfnzt},
    code = {https://github.com/congzhang365/rhythm.metrics},

    author = {Zhang, Cong},
    title = {{R Package “rhythm_metrics”}},
    year = {2022},
    
    abstract = {The rhythm.metrics package is designed for calculating and visualising speech rhythm metrics. This package provides the calculation of Delta C / Delta V, VarcoC / VarcoV, %V, rPVI_C, nPVI_V. },

}


@inproceedings{Gryllia2022,
    selected={false},
    bibtex_show={true},
    abbr={Speech Prosody}, 
    html = {https://doi.org/10.21437/SpeechProsody.2022-153},
    pdf = {https://www.isca-speech.org/archive/pdfs/speechprosody_2022/gryllia22_speechprosody.pdf},
    code = {https://osf.io/emcfg/},

    author = {Gryllia, Stella and Arvaniti, Amalia and Zhang, Cong and Marcoux, Katherine},
    booktitle = {Speech Prosody 2022},
    title = {{The many shapes of H*}},
    year = {2022}
    }

@inproceedings{Arvaniti2022,
    selected={false},
    bibtex_show={true},
    abbr={Speech Prosody}, 
    html = {https://doi.org/10.21437/SpeechProsody.2022-170},
    pdf = {https://www.isca-speech.org/archive/pdfs/speechprosody_2022/arvaniti22_speechprosody.pdf},
    code = {https://osf.io/wm7bc/},

    author = {Arvaniti, Amalia and Gryllia, Stella and Zhang, Cong and Marcoux, Katherine},
    booktitle = {Speech Prosody 2022},
    title = {{Disentangling emphasis from pragmatic contrastivity in the English H* $\sim$ L+H* contrast}},
    year = {2022}
}


# ------------------- 2021 ----------------
@article{zhu2021g2p,
    selected=True,
    bibtex_show={true},
    abbr={Interspeech},
    pdf={https://arxiv.org/pdf/2204.03067.pdf},
    code={https://github.com/lingjzhu/CharsiuG2P},

    title={ByT5 model for massively multilingual grapheme-to-phoneme conversion},
    author={Zhu, Jian and Zhang, Cong and Jurgens, David},
    year={2022},

    journal={Interspeech},
    
    abstract={In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P.}
  }


@article{zhu2021phone,
    selected={false},
    bibtex_show={true},
    abbr={ICASSP},
    pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746112},
    html={https://doi.org/10.1109/ICASSP43922.2022.9746112},
    code={https://github.com/lingjzhu/charsiu},

      title={Phone-to-audio alignment without text: A Semi-supervised Approach},
      author={Zhu, Jian and Zhang, Cong and Jurgens, David},
      journal={IEEE International Conference on Acoustics, Speech and Signal Processing},
      year={2022},
    
      abstract={The task of phone-to-audio alignment has many applications in speech research. Here we introduce two Wav2Vec2-based models for both text-dependent and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a semi-supervised model, directly learns phone-to-audio alignment through contrastive learning and a forward sum loss, and can be coupled with a pretrained phone recognizer to achieve text-independent alignment. The other model, Wav2Vec2-FC, is a frame classification model trained on forced aligned labels that can both perform forced alignment and text-independent segmentation. Evaluation results suggest that both proposed methods, even when transcriptions are not available, generate highly close results to existing forced alignment tools. Our work presents a neural pipeline of fully automated phone-to-audio alignment.}
}

@article{zhang21i_interspeech,
    selected={true},
    bibtex_show={true},
    abbr={Interspeech},

  author={Cong Zhang and Jian Zhu},
  title={{Synchronising Speech Segments with Musical Beats in Mandarin and English Singing}},
  year=2021,
  journal={Proc. Interspeech 2021},

  pages={1199--1203},
  doi={10.21437/Interspeech.2021-1841},

  pdf={https://www.isca-speech.org/archive/pdfs/interspeech_2021/zhang21i_interspeech.pdf},

  abstract="Generating synthesised singing voice with models trained on speech data has many advantages due to the models’ flexibility and controllability. However, since the information about the temporal relationship between segments and beats are lacking in speech training data, the synthesised singing may sound off-beat at times. Therefore, the availability of the information on the temporal relationship between speech segments and music beats is crucial. The current study investigated the segment-beat synchronisation in singing data, with hypotheses formed based on the linguistics theories of P-centre and sonority hierarchy. A Mandarin corpus and an English corpus of professional singing data were manually annotated and analysed. The results showed that the presence of musical beats was more dependent on segment duration than sonority. However, the sonority hierarchy and the P-centre theory were highly related to the location of beats. Mandarin and English demonstrated cross-linguistic variations despite exhibiting common patterns."
}

@article{Zhang2021b,
abbr = {JASA},
selected = True,
abstract = {Face-to-face speech data collection has been next to impossible globally due to COVID-19 restrictions. To address this problem, simultaneous recordings of three repetitions of the cardinal vowels were made using a Zoom H6 Handy Recorder with external microphone (henceforth H6) and compared with two alternatives accessible to potential participants at home: the Zoom meeting application (henceforth Zoom) and two lossless mobile phone applications (Awesome Voice Recorder, and Recorder; henceforth Phone). F0 was tracked accurately by all devices; however, for formant analysis (F1, F2, F3) Phone performed better than Zoom, i.e. more similarly to H6, though data extraction method (VoiceSauce, Praat) also resulted in differences. In addition, Zoom recordings exhibited unexpected drops in intensity. The results suggest that lossless format phone recordings present a viable option for at least some phonetic studies.},
author = {Zhang, Cong and Jepson, Kathleen and Lohfink, Georg and Arvaniti, Amalia},
doi = {10.1121/10.0005132},
file = {:C\:/Users/Cong/Dropbox/Oxford Research/Mendeley/Zhang et al. - 2021 - Comparing acoustic analyses of speech data collected remotely.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {3910--3916},
publisher = {Acoustical Society of America},
title = {{Comparing acoustic analyses of speech data collected remotely}},
volume = {149},
year = {2021}
}



@misc{Zhang2021a,
abbr = {TTS},
selected = True,
author = {Zhang, Cong and Zeng, Huinan},
doi = {10.5281/ZENODO.5553685},
keywords = {TTS,feature tts,phonological feature},
month = {oct},
title = {{Phonological feature mapping for FeatureTTS}},
url = {https://zenodo.org/record/5553685#.YV6Nw5pBzFQ},
year = {2021}
}

# ------------------- 2020 ----------------

@article{Zhang2020b,
author = {Zhang, Cong and Wang, Xinrong},
doi = {10.21437/speechprosody.2020-122},
file = {:C\:/Users/Cong/Dropbox/Oxford Research/Mendeley/Zhang, Wang - 2020 - Segment Duration and Proportion in Mandarin Singing.pdf:pdf},
number = {May},
pages = {596--600},
title = {{Segment Duration and Proportion in Mandarin Singing}},
year = {2020}
}


@inproceedings{Wright2015,
address = {Edinburgh, UK.},
author = {Wright, Clare and Zhang, Cong},
booktitle = {Proceeding of The Disfluency in Spontaneous Speech (DISS 2015) ICPhS Satelite Meeting},
editor = {Lickley, Robin},
file = {:C\:/Users/Cong/Dropbox/Oxford Research/Mendeley/Wright, Zhang - 2015 - The Effect of Study Abroad Experience on L2 Mandarin Disfluency in Different Types of Tasks.pdf:pdf},
keywords = {fluency,l2 mandarin,study abroad},
mendeley-tags = {fluency},
title = {{The Effect of Study Abroad Experience on L2 Mandarin Disfluency in Different Types of Tasks}},
year = {2015}
}



@phdthesis{Zhang2018a,
author = {Zhang, Cong},
keywords = {Doctoral thesis},
mendeley-tags = {Doctoral thesis},
school = {University of Oxford},
title = {{Tianjin Mandarin Tones and Tunes}},
year = {2018}
}

@inproceedings{Zhang2019a,
abbr = {ICPhS},
selected = True,
author = {Zhang, Cong},
booktitle = {Proceedings of the 19th ICPhS},
file = {:C\:/Users/Cong/Dropbox/Oxford Research/Mendeley/Zhang - 2019 - Stacking and Unstacking Prosodies The Production and Perception of Sentence Prosody in a Tonal Language(2).pdf:pdf},
keywords = {intonation,tianjin mandarin,tone},
title = {{Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language}},
url = {https://icphs2019.org/icphs2019-fullpapers/pdf/full-paper_930.pdf},
year = {2019}
}
@inproceedings{Zhang2017,
address = {Cologne, Germany},
author = {Zhang, Cong},
booktitle = {Phonetics and Phonology in Europe 2017},
title = {{Tianjin Mandarin Tunes: Production and Perception data}},
year = {2017}
}
@article{Zhang,
author = {Zhang, Cong and Lahiri, Aditi},
title = {{Floating Boundary Tone: Production and Perception of Syntactically Unmarked Polar Question in Tianjin Mandarin}}
}
@article{Wright2014,
author = {Wright, Clare and Zhang, Cong},
file = {:C\:/Users/Cong/Dropbox/Oxford Research/Mendeley/Wright, Zhang - 2014 - Examining the Effects of Study Abroad on Mandarin Chinese Language Development among UK University Learners.pdf:pdf},
journal = {Newcastle Working Papers in Linguistics},
keywords = {fluency},
mendeley-tags = {fluency},
pages = {67--84},
title = {{Examining the Effects of Study Abroad on Mandarin Chinese Language Development among UK University Learners}},
url = {http://www.ncl.ac.uk/linguistics/assets/documents/WrightandZhang.pdf},
volume = {20},
year = {2014}
}
@inproceedings{Zhang2020a,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy bynhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Zhang, Cong and Jepson, Kathleen and Lohfink, Georg and Arvaniti, Amalia},
booktitle = {179th Meeting of the Acoustical Society of America},
doi = {https://doi.org/10.1121/1.5147535},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Speech data collection at a distance: Comparing the reliability of acoustic cues across homemade recordings}},
year = {2020}
}
@inproceedings{Arvaniti2021,
author = {Arvaniti, Amalia and Jepson, Kathleen and Zhang, Cong and Marcoux, Katherine},
booktitle = {Interspeech 2021},
title = {{Intonation Transcription and Modelling in Research and Speech Technology Applications}},
url = {https://osf.io/2by9m/},
year = {2021}
}
@inproceedings{Zhang2018,
author = {Zhang, Cong},
booktitle = {9th International Conference on Speech Prosody 2018},
doi = {10.21437/SpeechProsody.2018-106},
month = {jun},
pages = {522--526},
publisher = {ISCA},
title = {{Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls}},
url = {http://www.isca-speech.org/archive/SpeechProsody_2018/abstracts/190.html},
year = {2018}
}
@techreport{Zhang2016a,
abstract = {Zhang, C. 2016. Tones and Tunes in Tianjin Mandarin. TIE 2016 (Tone and Intonation in Europe 2016), 1-3 September, 2016. University of Kent, UK.},
address = {University of Kent, UK.},
author = {Zhang, Cong},
booktitle = {Tone and Intonation in Europe 2016},
publisher = {1-3 September, 2016. University of Kent, UK.},
title = {{Tones and Tunes in Tianjin Mandarin}},
year = {2016}
}



@article{Zhang2014a,
author = {Zhang, Cong},
journal = {St. Anne's Annual Review},
keywords = {teaching},
mendeley-tags = {teaching},
pages = {105--125},
title = {{The effect of immediate feedback on the perception of Mandarin lexical tones by non-native speakers of Mandarin}},
volume = {5},
year = {2014}
}



@inproceedings{Zhang2021,
abstract = {This study investigates whether phonological features can be applied in text-to-speech systems to generate native and non-native speech. We present a mapping between ARPABET/pinyin->SAMPA/SAMPA-SC->phonological features in this paper, and tested whether native, non-native, and code-switched speech could be successfully generated using this mapping. We ran two experiments, one with a small dataset and one with a larger dataset. The results proved that phonological features can be a feasible input system, although it needs further investigation to improve model performance. The accented output generated by the TTS models also helps with understanding human second language acquisition processes.},
archivePrefix = {arXiv},
arxivId = {2110.03609},
author = {Zhang, Cong and Zeng, Huinan and Liu, Huang and Zheng, Jiewen},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
eprint = {2110.03609},

month = {oct},
title = {{Applying Phonological Features in Multilingual Text-To-Speech}},
url = {https://arxiv.org/abs/2110.03609},
year = {2021}
}
